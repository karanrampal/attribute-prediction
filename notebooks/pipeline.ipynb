{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68421e9f-fca0-4825-94eb-cd6ef23bc36e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277952ea-e366-4722-9ced-a8977132bc5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5664d4f8-adfc-4f1e-9d64-bc1fce5eb290",
   "metadata": {},
   "source": [
    "# Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fba1b-fa51-4024-a5cd-3f567553469b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"airesearch-1409\"\n",
    "REGION = \"europe-west4\"\n",
    "IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12:latest\"\n",
    "BUCKET_NAME = \"gs://attributes_models/base_model\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f7899-5a6a-4d31-a78e-28ce2e80ae56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"BUCKET_NAME\"] = BUCKET_NAME\n",
    "os.environ[\"SYS_EXE_PY_CONDA\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1b7c9-7530-45f3-9c08-ed132f5bc1f2",
   "metadata": {},
   "source": [
    "# Make package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ed932-8ec0-4f61-8f51-b251beaeb4d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd ../\n",
    "$SYS_EXE_PY_CONDA -m build\n",
    "gsutil cp ./dist/*.whl $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d7b23-ae88-4795-8b9c-f5e6abce046d",
   "metadata": {},
   "source": [
    "# Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e60e27-7105-42c9-a69f-20357801b893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\"])\n",
    "def create_castors(data_root: str) -> str:\n",
    "    \"\"\"Create castors data csv file\"\"\"\n",
    "    from glob import glob\n",
    "    import os\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    path_ = os.path.join(data_root, \"annotations/castors.csv\")\n",
    "    if not os.path.isfile(path_):\n",
    "        num = len(\"/gcs/hm_images/images/\")\n",
    "\n",
    "        file_list = glob(data_root + \"/images/**/*.jpg\", recursive=True)\n",
    "        castors = [int(os.path.basename(path)[:-4]) for path in file_list]\n",
    "        path_list = [path[num:] for path in file_list]\n",
    "\n",
    "        out = pd.DataFrame(data={\"path\": path_list, \"castor\": castors})\n",
    "\n",
    "        out.to_csv(path_, index=False)\n",
    "\n",
    "    return path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1387907-3052-482c-8315-7568234e3333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn\"])\n",
    "def create_datasets(data_root: str, castors_path: str, out_path: str) -> str:\n",
    "    \"\"\"Create training and test data csv file\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "    cols = [\n",
    "        \"product_code\",\n",
    "        \"article_code\",\n",
    "        \"product_age_group\",\n",
    "        \"product_waist_rise\",\n",
    "        \"product_sleeve_length\",\n",
    "        \"product_garment_length\",\n",
    "        \"product_fit\",\n",
    "        \"product_sleeve_style\",\n",
    "        \"product_neck_line_style\",\n",
    "        \"product_collar_style\",\n",
    "        \"article_presentation_color_group\",\n",
    "    ]\n",
    "\n",
    "    if not os.path.isfile(os.path.join(out_path, \"annotations/train.csv\")):\n",
    "        padma = pd.read_parquet(\n",
    "            os.path.join(data_root, \"dma/product_article_datamart.parquet\"),\n",
    "            columns=[\"product_code\", \"article_code\", \"castor\"]\n",
    "        )\n",
    "        pim = pd.read_parquet(os.path.join(data_root, \"dim/dim_pim.parquet\"), columns=cols)\n",
    "        castors = pd.read_csv(castors_path)\n",
    "\n",
    "        # Clean data tables\n",
    "        padma = padma.drop_duplicates()\n",
    "        padma.castor = padma.castor.astype(int)\n",
    "\n",
    "        pim = pim.dropna(axis=0, subset=[\"article_code\"])\n",
    "        pim = pim.drop_duplicates()\n",
    "\n",
    "        # Process PIM data\n",
    "        out = []\n",
    "        for col in cols:\n",
    "            out.append(pim[col].apply(lambda x: json.loads(x) if x and \"[\" in x else x))\n",
    "        tmp = pd.concat(out, axis=1)\n",
    "        out = []\n",
    "        for col in cols[2:]:\n",
    "            out.append(pd.get_dummies(tmp[col].explode()).reset_index().groupby(\"index\").max())\n",
    "        res = pd.concat(out, axis=1)\n",
    "        res = pd.concat([pim[cols[:2]], res], axis=1)\n",
    "\n",
    "        # Merge pim and padma table\n",
    "        data = res.merge(padma, on=[\"product_code\", \"article_code\"], how=\"left\")\n",
    "        data.dropna(inplace=True)\n",
    "        data = data.drop(axis=1, labels=[\"product_code\", \"article_code\"])\n",
    "\n",
    "        # Merge castor data to get output\n",
    "        out = castors.merge(data, on=\"castor\", how=\"inner\")\n",
    "\n",
    "        # Split data into training and test dataset\n",
    "        gss = GroupShuffleSplit(n_splits=1, train_size=0.9, random_state=42)\n",
    "        train_idxs, test_idxs = next(gss.split(X=out.path, groups=out.castor))\n",
    "        out.drop(columns=[\"castor\"], axis=1, inplace=True)\n",
    "        train = out.iloc[train_idxs, :]\n",
    "        test = out.iloc[test_idxs, :]\n",
    "\n",
    "        # Write output files\n",
    "        train.to_csv(os.path.join(out_path, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(out_path, \"test.csv\"), index=False)\n",
    "    return os.path.basename(castors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ce36e-d2c8-4bf0-bb57-7de8c449bb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=IMAGE_URI)\n",
    "def train(project_id: str, region: str, bucket_name: str, data_dir: str) -> None:\n",
    "    \"\"\"Training component\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    SERVICE_ACCOUNT = \"vertex-ai-training@airesearch-1409.iam.gserviceaccount.com\"\n",
    "    IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-12:latest\"\n",
    "    TB_RESOURCE_NAME = f\"projects/184243724142/locations/{region}/tensorboards/4596222486894346240\"\n",
    "\n",
    "    DISPLAY_NAME = \"attributes_model\"\n",
    "    MODULE_NAME = \"trainer.train\"\n",
    "    GCS_OUTPUT_URI_PREFIX = f\"{bucket_name}\"\n",
    "    PACKAGE_NAME = \"product_attributes-0.0.1-py3-none-any.whl\"\n",
    "\n",
    "    REPLICA_COUNT = 2\n",
    "    MACHINE_TYPE = \"n1-standard-4\"\n",
    "    ACCELERATOR_COUNT = 1\n",
    "    ACCELERATOR_TYPE = \"NVIDIA_TESLA_T4\"\n",
    "    ARGS = [\"--batch_size\", \"128\", \"--num_epochs\", \"2\"]\n",
    "\n",
    "    aiplatform.init(project=project_id, staging_bucket=bucket_name, location=region)\n",
    "    tensorboard = aiplatform.Tensorboard(TB_RESOURCE_NAME)\n",
    "\n",
    "    custom_training_job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        python_package_gcs_uri= f\"{bucket_name}/{PACKAGE_NAME}\",\n",
    "        python_module_name=MODULE_NAME,\n",
    "        container_uri=IMAGE_URI,\n",
    "    )\n",
    "\n",
    "    vertex_model = custom_training_job.run(\n",
    "        args=ARGS,\n",
    "        base_output_dir=GCS_OUTPUT_URI_PREFIX,\n",
    "        replica_count=REPLICA_COUNT,\n",
    "        machine_type=MACHINE_TYPE,\n",
    "        accelerator_count=ACCELERATOR_COUNT,\n",
    "        accelerator_type=ACCELERATOR_TYPE,\n",
    "        tensorboard=tensorboard.resource_name,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        enable_web_access=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308481a4-af78-417d-ae5e-ee77f7dbae8c",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e88c4-3e32-41e7-9d5c-77afbbcdc795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"attributes-model-pipeline\", pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(project_id: str, region: str, bucket_name: str):\n",
    "    \"\"\"Attribute prediction training pipeline\n",
    "    Args:\n",
    "        project_id: Id for the GCP project\n",
    "        region: Region in GCP\n",
    "        data_root: Root directory for data images, annotations etc.\n",
    "    \"\"\"\n",
    "    castor_task = create_castors(\"/gcs/hm_images\")\n",
    "    dataset_task = create_datasets(\n",
    "        \"/gcs/hdl_tables\", castor_task.output, \"/gcs/hm_images/annotations\"\n",
    "    )\n",
    "    train_task = train(project_id, region, bucket_name, dataset_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d69a1-a31b-4aac-a7d9-bdcf37f20630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"attributes_model_pipeline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e8728-fb1b-47df-b93c-7dc3645e88d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"attributes-model-pipeline\",\n",
    "    template_path=\"attributes_model_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project_id\": PROJECT_ID,\n",
    "        \"region\": REGION,\n",
    "        \"bucket_name\": BUCKET_NAME,\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ed483-f767-4150-b3b3-da7eee881ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cv3d-env",
   "name": "pytorch-gpu.1-13.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m106"
  },
  "kernelspec": {
   "display_name": "cv3d-env",
   "language": "python",
   "name": "cv3d-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
